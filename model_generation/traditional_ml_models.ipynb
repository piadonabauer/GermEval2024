{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning Models\n",
    "\n",
    "This notebook focuses on implementing traditional machine learning models, each tailored to a specific annotator. The models are solely trained on the annotator's labeled data and undergo hyperparameter fine-tuning before.\n",
    "\n",
    "Additionally, the performance of different feature extraction approaches on the dataset is compared, namely CountVectorizer, TfidfVectorizer, and Transformer ([BERT](https://huggingface.co/dbmdz/bert-base-german-cased)).\n",
    "\n",
    "Training is conducted on a dataset augmented twice. The training process is conducted in two stages: initially, a binary classification determines whether a data point exhibits sexism, followed by a more detailed classification of the level of sexism for each predicted instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Installation of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!pip install torch -q\n",
    "!pip install imbalanced-learn -q\n",
    "!pip install xgboost -q\n",
    "!pip install lightgbm -q\n",
    "!pip install catboost -q\n",
    "!pip install mlxtend -q\n",
    "!pip install tqdm -q\n",
    "!pip install gensim -q\n",
    "!pip install transformers -q\n",
    "!python -m spacy download de_core_news_md -q\n",
    "!pip install spacy -q\n",
    "!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import mlxtend\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from typing import Dict\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['LOGGING_LEVEL'] = 'INFO' \n",
    "logging.basicConfig(level=os.environ.get('LOGGING_LEVEL', 'WARNING'),\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATOR_COLUMNS = ['A001', 'A002', 'A003', 'A004', 'A005', 'A007', 'A008', 'A009', 'A010', 'A012']\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "checkpoint = \"google-bert/bert-base-german-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_bert = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text):\n",
    "    doc = nlp(text)\n",
    "    result = ' '.join([\n",
    "        token.text if token.is_punct else token.lemma_\n",
    "        for token in doc\n",
    "        if not token.is_stop\n",
    "    ])\n",
    "    return result.lower()\n",
    "\n",
    "def get_data(classes, lemmat, path):\n",
    "    \"\"\"\n",
    "    Processes the input data based on specified classes and lemmatization option.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df = df[:100]\n",
    "    \n",
    "    if lemmat:\n",
    "        df['text'] = df['text'].apply(lambda x: lemma(x))\n",
    "    \n",
    "    if classes==2:\n",
    "        df[ANNOTATOR_COLUMNS] = df[ANNOTATOR_COLUMNS].replace([2, 3, 4], 1)\n",
    "\n",
    "    elif classes==4:\n",
    "        replace_dict = {0: -1, 1: 0, 2: 1, 3: 2, 4: 3}\n",
    "        df[ANNOTATOR_COLUMNS] = df[ANNOTATOR_COLUMNS].replace(replace_dict)\n",
    "\n",
    "    elif classes != 5:\n",
    "        print('Specify right nr of classes!')\n",
    "    \n",
    "    dataframes = {}\n",
    "\n",
    "    for col in ANNOTATOR_COLUMNS:\n",
    "        df_task = df[['text', col]].dropna()\n",
    "        df_task = df_task[df_task[col] != -1]\n",
    "        dataframes[col] = df_task\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('german')\n",
    "\n",
    "def NLP1(text):\n",
    "    text_tok = word_tokenize(text)\n",
    "    \n",
    "    eng_stopwords = stopwords.words('german')\n",
    "    text_stp = [word for word in text_tok if (word.lower() not in eng_stopwords) and word.isalpha()]\n",
    "    \n",
    "    stemmer = SnowballStemmer(language='german')\n",
    "    text_stm = [stemmer.stem(word) for word in text_stp]\n",
    "    return text_stm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses pickle to save and load vectorizers to/from disk.\n",
    "vecs = {\n",
    "    'A001': None, 'A002': None, 'A003': None, 'A004': None, 'A005': None, 'A007': None,\n",
    "     'A008': None, 'A009': None, 'A010': None, 'A012': None\n",
    "} \n",
    "\n",
    "def get_text_features(df, method, train, anno):\n",
    "    \"\"\"\n",
    "    Generate text features from the dataset using different vectorization methods (indicated by parameter).\n",
    "    \"\"\"\n",
    "    vectorizer_filename = f'{anno}_{method.lower()}_vectorizer.pkl'\n",
    "    \n",
    "    if method == 'CountVectorizer':\n",
    "        if train:\n",
    "            vectorizer = CountVectorizer()\n",
    "            X = vectorizer.fit_transform(df['text'])\n",
    "            with open(vectorizer_filename, 'wb') as f:\n",
    "                pickle.dump(vectorizer, f)\n",
    "            vecs[anno] = vectorizer\n",
    "            return X\n",
    "        else:\n",
    "            if vecs[anno] is None:\n",
    "                with open(vectorizer_filename, 'rb') as f:\n",
    "                    vecs[anno] = pickle.load(f)\n",
    "            return vecs[anno].transform(df['text'])\n",
    "    \n",
    "    elif method == 'TfidfVectorizer':\n",
    "        if train:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            X = vectorizer.fit_transform(df['text'])\n",
    "            with open(vectorizer_filename, 'wb') as f:\n",
    "                pickle.dump(vectorizer, f)\n",
    "            vecs[anno] = vectorizer\n",
    "            return X\n",
    "        else:\n",
    "            if vecs[anno] is None:\n",
    "                with open(vectorizer_filename, 'rb') as f:\n",
    "                    vecs[anno] = pickle.load(f)\n",
    "            return vecs[anno].transform(df['text'])\n",
    "    \n",
    "    elif method == 'Transformer':\n",
    "        \n",
    "        inputs = tokenizer(df, return_tensors='pt', max_length=64, truncation=True, padding='max_length')\n",
    "        inputs.to(device)\n",
    "        model_bert.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(df, anno):\n",
    "    \"\"\"\n",
    "    Computes class weights for a specific annotator's column in the DataFrame.\n",
    "    \"\"\"\n",
    "    y = df[anno].values\n",
    "    classes = np.unique(y)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    \n",
    "    return dict(zip(classes, class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Methods for each Model\n",
    "\n",
    "The selected models are:\n",
    "- Random Forest\n",
    "- Extreme Gradient Boosting (XGB)\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "- Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, class_weights, anno, method, paras):\n",
    "    \n",
    "   # if method=='Transformer':\n",
    "   #     X_train = np.array([np.array(x) for x in X_train])\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42,\n",
    "                                   class_weight=class_weights,\n",
    "                                   bootstrap=paras['bootstrap'],\n",
    "                                   max_depth=paras['max_depth'],\n",
    "                                   min_samples_leaf=paras['min_samples_leaf'],\n",
    "                                   min_samples_split=paras['min_samples_split'],\n",
    "                                   n_estimators=paras['n_estimators']\n",
    "                                  )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_xgboost(X_train, y_train, anno, paras):\n",
    "    if anno=='A002':\n",
    "        X_train = np.array(X_train.tolist())\n",
    "    \n",
    "    model = xgb.XGBClassifier(random_state=42,\n",
    "                              colsample_bytree=paras['colsample_bytree'],\n",
    "                              learning_rate=paras['learning_rate'],\n",
    "                              max_depth=paras['max_depth'],\n",
    "                              n_estimators=paras['n_estimators'],\n",
    "                              subsample=paras['subsample']\n",
    "                             )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_lightgbm(X_train, y_train, class_weights, anno, paras):\n",
    "    X_train = X_train.astype('float32')\n",
    "    \n",
    "    model = lgb.LGBMClassifier(class_weight=class_weights,\n",
    "                               max_depth=paras['max_depth'],\n",
    "                               learning_rate=paras['learning_rate'],\n",
    "                               n_estimators=paras['n_estimators'],\n",
    "                               verbose=-1\n",
    "                              )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_catboost(X_train, y_train, class_weights, anno, paras):\n",
    "    \n",
    "    model = CatBoostClassifier(iterations=paras['iterations'],\n",
    "                               learning_rate=paras['learning_rate'],\n",
    "                               depth=paras['depth'],\n",
    "                               logging_level='Silent',\n",
    "                               class_weights=class_weights\n",
    "                              )\n",
    "    \n",
    "    model.fit(X_train, y_train, verbose=True)\n",
    "    return model\n",
    "\n",
    "def train_svm(X_train, y_train, class_weights, anno, paras):\n",
    "    \n",
    "    model = SVC(C=paras['C'],\n",
    "                kernel=paras['kernel'],\n",
    "                gamma=paras['gamma'],\n",
    "                class_weight=class_weights,\n",
    "                verbose=False \n",
    "               )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, method, train, anno):\n",
    "    if method=='Transformer':\n",
    "        df['text_features'] = df['text'].apply(lambda text: get_text_features(df=text, method='Transformer', train=True, anno=anno))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df['text_features'].tolist(), df[anno], test_size=0.15, random_state=42)\n",
    "        #X_train = df['text_features'].tolist()\n",
    "        #y_train = df[anno]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "        #X_train = df['text'].apply(lambda text: get_text_features(text, method='Transformer', train=train, anno=anno))\n",
    "        #y_train = df[anno].values\n",
    "        #return X_train, y_train\n",
    "    \n",
    "    else:\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "        X_train = get_text_features(train_df, method=method, train=True, anno=anno)\n",
    "        X_test = get_text_features(test_df, method=method, train=False, anno=anno)\n",
    "        y_train = train_df[anno].values\n",
    "        y_test = test_df[anno].values\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determined the optimal model, feature extraction technique, and parameter combinations using grid search \n",
    "for each annotator individually. Results were saved in dictionaries to ensure reproducibility, and findings \n",
    "were categorized for both binary and multi-class classifications.\n",
    "\"\"\"\n",
    "\n",
    "best_combis_bin = {\n",
    "    'A001': {'method': 'Transformer', 'model': 'Catboost', \n",
    "            'paras': {'depth': 6, 'iterations': 500, 'learning_rate': 0.01}},\n",
    "    'A002': {'method': 'CountVectorizer', 'model': 'Rf', \n",
    "            'paras': {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}},\n",
    "    'A003': {'method': 'Transformer', 'model': 'Rf', \n",
    "            'paras': {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 500}},\n",
    "    'A004': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}},\n",
    "    'A005': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}},\n",
    "    'A007': {'method': 'TfidfVectorizer', 'model': 'SVM', \n",
    "            'paras': {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}},\n",
    "    'A008': {'method': 'CountVectorizer', 'model': 'SVM', \n",
    "            'paras': {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}},\n",
    "    'A009': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.8}},\n",
    "    'A010': {'method': 'Transformer', 'model': 'Light', \n",
    "            'paras': {'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 100}},\n",
    "    'A012': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.8}}\n",
    "}\n",
    "\n",
    "best_combis_multi = {\n",
    "    'A001': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}},\n",
    "    'A002': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8}},\n",
    "    'A003': {'method': 'TfidfVectorizer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}},\n",
    "    'A004': {'method': 'CountVectorizer', 'model': 'Rf', \n",
    "            'paras': {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}},\n",
    "    'A005': {'method': 'Transformer', 'model': 'Rf', \n",
    "            'paras': {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}},\n",
    "    'A007': {'method': 'CountVectorizer', 'model': 'Light', \n",
    "            'paras': {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}},\n",
    "    'A008': {'method': 'Transformer', 'model': 'Xgb', \n",
    "            'paras': {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}},\n",
    "    'A009': {'method': 'TfidfVectorizer', 'model': 'Light', \n",
    "            'paras': {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100}},\n",
    "    'A010': {'method': 'CountVectorizer', 'model': 'Rf', \n",
    "            'paras': {'bootstrap': False, 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}},\n",
    "    'A012': {'method': 'CountVectorizer', 'model': 'Rf', \n",
    "            'paras': {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, method, model_type='traditional'):\n",
    "        if method != 'Transformer':\n",
    "            X_test = X_test.astype('float32')\n",
    "            \n",
    "        #X_test = X_test.reshape(-1, 1)\n",
    "        if model_type == 'traditional':\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "        accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "        precision = round(precision_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "        recall = round(recall_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "        f1 = round(f1_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "    \n",
    "        return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, method):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the trained model on the test set. \n",
    "    \"\"\"\n",
    "    \n",
    "    if method != 'Transformer':\n",
    "        X_test = X_test.astype('float32')\n",
    "\n",
    "    #X_test = X_test.reshape(-1, 1)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    precision = round(precision_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "    recall = round(recall_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(dataframes, best_combis):  \n",
    "    \"\"\"\n",
    "    Trains models for each annotator based on the best parameter config from grid search.\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    for anno, df in dataframes.items():\n",
    "        print(f\"Training model(s) for Annotator {anno}...\")\n",
    "\n",
    "        method = best_combis[anno]['method']\n",
    "        model_name = best_combis[anno]['model']\n",
    "        paras = best_combis[anno]['paras']\n",
    "        print(f'Method: {method}')\n",
    "\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        class_weights_dict = compute_class_weights(df, anno)\n",
    "        X_train, X_test, y_train, y_test = get_features(df, method, True, anno)\n",
    "\n",
    "        if method == 'Transformer':\n",
    "            X_train = np.concatenate(X_train, axis=0)\n",
    "            X_test = np.concatenate(X_test, axis=0)\n",
    "            \n",
    "        model = None\n",
    "        \n",
    "        if model_name=='Rf':\n",
    "            model = train_random_forest(X_train, y_train, class_weights_dict, anno, method, paras)\n",
    "        elif model_name=='Xgb':\n",
    "            model = train_xgboost(X_train, y_train, anno, paras)\n",
    "        elif model_name=='Light':\n",
    "            model = train_lightgbm(X_train, y_train, class_weights_dict, anno, paras)\n",
    "        elif model_name=='Catboost':\n",
    "            model =  train_catboost(X_train, y_train, class_weights_dict, anno, paras)\n",
    "        elif model_name=='SVM':\n",
    "            model = train_svm(X_train, y_train, class_weights_dict, anno, paras)\n",
    "\n",
    "        metrics = evaluate_model(model, X_test, y_test, method)\n",
    "        print(metrics)\n",
    "\n",
    "        models[anno] = {\n",
    "            'Model name': model_name,\n",
    "            'Model': model,\n",
    "            'Text feature method': method,\n",
    "            'Metrics': metrics\n",
    "        }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model(s) for Annotator A001...\n",
      "Method: Transformer\n",
      "(1.0, 1.0, 1.0, 1.0)\n",
      "Training model(s) for Annotator A002...\n",
      "Method: CountVectorizer\n",
      "(0.867, 0.933, 0.5, 0.464)\n",
      "Training model(s) for Annotator A003...\n",
      "Method: Transformer\n",
      "(0.8, 0.875, 0.75, 0.762)\n",
      "Training model(s) for Annotator A004...\n",
      "Method: Transformer\n",
      "(0.75, 0.875, 0.5, 0.429)\n",
      "Training model(s) for Annotator A005...\n",
      "Method: Transformer\n",
      "(0.333, 0.25, 0.25, 0.25)\n",
      "Training model(s) for Annotator A007...\n",
      "Method: TfidfVectorizer\n",
      "(0.5, 0.75, 0.5, 0.333)\n",
      "Training model(s) for Annotator A008...\n",
      "Method: CountVectorizer\n",
      "(0.6, 0.8, 0.5, 0.375)\n",
      "Training model(s) for Annotator A009...\n",
      "Method: Transformer\n",
      "(0.875, 0.917, 0.833, 0.855)\n",
      "Training model(s) for Annotator A010...\n",
      "Method: Transformer\n",
      "(0.6, 0.568, 0.556, 0.55)\n",
      "Training model(s) for Annotator A012...\n",
      "Method: Transformer\n",
      "(0.667, 0.667, 0.611, 0.603)\n"
     ]
    }
   ],
   "source": [
    "path = 'df_train_original.csv'\n",
    "classes = 2\n",
    "lemmat = False\n",
    "dataframes = get_data(classes=classes, lemmat=lemmat, path=path)\n",
    "best_combis = best_combis_bin if classes == 2 else best_combis_multi\n",
    "\n",
    "models = train_models(dataframes, best_combis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply trained Models for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_update(df, models, binary_prediction):\n",
    "    \"\"\"\n",
    "    Predicts values for cells in the dataframe based on trained models stored in 'models'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_predicted = df.copy()\n",
    "\n",
    "    # Iterate over each annotator and model\n",
    "    for anno, model_info in tqdm(models.items(), desc='Annotators', total=len(models)):\n",
    "\n",
    "        model = model_info['Model']\n",
    "        method = model_info['Text feature method']\n",
    "\n",
    "        if method == 'CountVectorizer':\n",
    "            text_features = get_text_features(df, method='CountVectorizer', train=False, anno=anno).toarray()\n",
    "        elif method == 'TfidfVectorizer':\n",
    "            text_features = get_text_features(df, method='TfidfVectorizer', train=False, anno=anno).toarray()\n",
    "        elif method == 'Transformer':\n",
    "            text_features = df['text'].apply(lambda text: get_text_features(df=text, method='Transformer', train=False, anno=anno))\n",
    "            text_features = list(text_features)\n",
    "\n",
    "        for idx, value in enumerate(df[anno]):\n",
    "            if value == 1:   \n",
    "                text_to_predict = text_features[idx]\n",
    "                if method=='Transformer':\n",
    "                    text_to_predict = np.concatenate(text_to_predict, axis=0)\n",
    "                predicted_value = model.predict([text_to_predict])[0]\n",
    "                \n",
    "                if binary_prediction:\n",
    "                    df_predicted.loc[idx, anno] = predicted_value #+ 1  # Increment by 1 as requested\n",
    "                else: \n",
    "                    df_predicted.loc[idx, anno] = predicted_value + 1\n",
    "                    \n",
    "            if binary_prediction:\n",
    "                df_predicted.loc[idx, anno] = -1\n",
    "                \n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotators: 10it [00:52,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('df_comp_bin_3.csv')\n",
    "df_new = predict_and_update(df_test, models)\n",
    "df_new = df_new.drop('Unnamed: 0', axis=1)\n",
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>A008</th>\n",
       "      <th>A007</th>\n",
       "      <th>A003</th>\n",
       "      <th>A005</th>\n",
       "      <th>A004</th>\n",
       "      <th>A012</th>\n",
       "      <th>A009</th>\n",
       "      <th>A002</th>\n",
       "      <th>A001</th>\n",
       "      <th>A010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f3b81af2f6852bf1b9896629525d2f41</td>\n",
       "      <td>Ja, Frauen können krankhaft eifersüchtig werde...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cf8b8bac7165144bb62b399a98843366</td>\n",
       "      <td>Ich hau' auf jede Religion gern drauf, aber de...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0c45cdf4cca5eec566d6dd53653b532b</td>\n",
       "      <td>Wow, Vorstadtmama, perfekte Dransleischen. Und...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3a60877d2c04ba65f457f7cc3e003169</td>\n",
       "      <td>gratuliere USA\\ndie erste schwarze &amp; frau als ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f389b63364d8da93860e3c7e6569bf5b</td>\n",
       "      <td>Frauen wählten mehrheitlich Biden ...\\nwürden ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>2f7322c62b63ff74ec945bb38ed9f258</td>\n",
       "      <td>Ihre Partnerin schämt sich wahrscheinlich für ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>ec5fe35f542aac2f3155177dbf2731c2</td>\n",
       "      <td>glaube ich diese These einfach nicht.Grund: He...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>6674986a02bab67b011df90cc7396a96</td>\n",
       "      <td>Damit die Ehefrau dann ein Dauervisum erhält m...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2a3774eba33afe18af2f0d312d081bb3</td>\n",
       "      <td>Ich selbst habe in meiner Hierarchie zwei Frau...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>43891e25f522166dda98045a33563c05</td>\n",
       "      <td>Ich finde die Nahaufnahmen  und sexistische Be...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1986 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0     f3b81af2f6852bf1b9896629525d2f41   \n",
       "1     cf8b8bac7165144bb62b399a98843366   \n",
       "2     0c45cdf4cca5eec566d6dd53653b532b   \n",
       "3     3a60877d2c04ba65f457f7cc3e003169   \n",
       "4     f389b63364d8da93860e3c7e6569bf5b   \n",
       "...                                ...   \n",
       "1981  2f7322c62b63ff74ec945bb38ed9f258   \n",
       "1982  ec5fe35f542aac2f3155177dbf2731c2   \n",
       "1983  6674986a02bab67b011df90cc7396a96   \n",
       "1984  2a3774eba33afe18af2f0d312d081bb3   \n",
       "1985  43891e25f522166dda98045a33563c05   \n",
       "\n",
       "                                                   text  A008  A007  A003  \\\n",
       "0     Ja, Frauen können krankhaft eifersüchtig werde...     2     4     4   \n",
       "1     Ich hau' auf jede Religion gern drauf, aber de...     0     0     2   \n",
       "2     Wow, Vorstadtmama, perfekte Dransleischen. Und...     0     4     4   \n",
       "3     gratuliere USA\\ndie erste schwarze & frau als ...     0     0     0   \n",
       "4     Frauen wählten mehrheitlich Biden ...\\nwürden ...     4     0     4   \n",
       "...                                                 ...   ...   ...   ...   \n",
       "1981  Ihre Partnerin schämt sich wahrscheinlich für ...    -1    -1    -1   \n",
       "1982  glaube ich diese These einfach nicht.Grund: He...    -1    -1    -1   \n",
       "1983  Damit die Ehefrau dann ein Dauervisum erhält m...    -1    -1    -1   \n",
       "1984  Ich selbst habe in meiner Hierarchie zwei Frau...    -1    -1    -1   \n",
       "1985  Ich finde die Nahaufnahmen  und sexistische Be...    -1    -1    -1   \n",
       "\n",
       "      A005  A004  A012  A009  A002  A001  A010  \n",
       "0        0     0     2     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0  \n",
       "2        0     0     2     0     0     2     3  \n",
       "3        0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...  \n",
       "1981    -1    -1    -1     0     0    -1     2  \n",
       "1982    -1    -1    -1     0     0    -1     0  \n",
       "1983    -1    -1    -1     0     0    -1     0  \n",
       "1984    -1    -1    -1     0     0    -1     0  \n",
       "1985    -1    -1    -1     0     0    -1     3  \n",
       "\n",
       "[1986 rows x 12 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_new.to_csv('df_com_bin_3.csv')\n",
    "#df_new.to_csv('df_comp_multi_3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_df(): # to predict df, has 1 and NaN\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    df_test = df_test.drop('Unnamed: 0', axis=1)\n",
    "    df_test.replace(0, np.nan, inplace=True) # replace 0 with NaN \n",
    "    valid_counts = df_test[ANNOTATOR_COLUMNS].apply(lambda x: x.notna()) # add nr annos\n",
    "    df_test['N'] = valid_counts.sum(axis=1)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, df, text_vectorized, bin):\n",
    "    #df[ANNOTATOR_COLUMNS] = df[ANNOTATOR_COLUMNS].replace(0, -1)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), position=0, leave=True, desc=\"Predicting\"):\n",
    "        for annotator in ANNOTATOR_COLUMNS:\n",
    "            \n",
    "            if bin:\n",
    "                if not pd.isna(row[annotator]): # dataframe has 1 and NaN\n",
    "                    model = models.get(annotator)\n",
    "                    if model is not None:\n",
    "                        pred = model.predict(text_vectorized[index].reshape(1, -1)) \n",
    "                        df.at[index, annotator] = pred[0]     \n",
    "                        \n",
    "            else: \n",
    "                 if row[annotator] == -1 or row[annotator] == -1.0: # weil um -2 dekr, eig 1, also eig sexis\n",
    "                    model = models.get(annotator)\n",
    "                    if model is not None:\n",
    "                        pred = model.predict(text_vectorized[index].reshape(1, -1)) \n",
    "                        df.at[index, annotator] = pred[0]  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, df_mul = get_train_df(split=3, path='df_train_original.csv', upsample=False, lemmat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Other code used for hyperparameter tuning via grid search.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=1),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=1),\n",
    "    'f1': make_scorer(f1_score, average='weighted', zero_division=1)\n",
    "}\n",
    "\n",
    "def hyperparameter_search_rf(X_train, y_train, class_weights):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'min_samples_leaf': [1, 4, 10],\n",
    "        'bootstrap': [False],\n",
    "        'class_weight': [class_weights]\n",
    "    }\n",
    "    \n",
    "    # if method=='Transformer':\n",
    "    #    X_train = np.array([np.array(x) for x in X_train])\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, scoring=scoring, refit='accuracy', n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for RandomForest: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for RandomForest: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def hyperparameter_search_xgb(X_train, y_train):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, scoring=scoring, refit='accuracy', n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for XGBoost: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for XGBoost: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def hyperparameter_search_lgb(X_train, y_train, class_weights):\n",
    "    X_train = X_train.astype('float32')\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'class_weight': [class_weights]\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(verbose=-1)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, scoring=scoring, refit='accuracy', n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for LightGBM: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for LightGBM: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def hyperparameter_search_cb(X_train, y_train, class_weights=None):\n",
    "    param_grid = {\n",
    "        'iterations': [750],\n",
    "        'depth': [5],\n",
    "        'learning_rate': [0.05],\n",
    "        'class_weights': [class_weights]\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(metric_period=1000)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, scoring=scoring, refit='accuracy', n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for CatBoost: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for CatBoost: {grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = get_data(classes=4, lemmat=False, path='df_train_original.csv')\n",
    "results = {}\n",
    "\n",
    "for anno, df in dataframes.items():\n",
    "    print(f\"Training models for Annotator {anno}...\")\n",
    "    \n",
    "    methods = ['TfidfVectorizer', 'CountVectorizer', 'TfidfVectorizer']\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = get_features(df, method, True, anno)\n",
    "\n",
    "        # for multi\n",
    "        if method == 'Transformer':\n",
    "            X_train = np.concatenate(X_train, axis=0)\n",
    "            X_test = np.concatenate(X_test, axis=0)\n",
    "\n",
    "        class_weights_dict = compute_class_weights(df, anno)\n",
    "        model = None\n",
    "\n",
    "        def evaluate_model(model, X_test, y_test, model_type='traditional'):\n",
    "            if method != 'Transformer':\n",
    "                X_test = X_test.astype('float32')\n",
    "            if model_type == 'traditional':\n",
    "                X_test = X_test[0]\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "            accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "            precision = round(precision_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "            recall = round(recall_score(y_test, y_pred, average='macro', zero_division=1), 3)\n",
    "\n",
    "            return accuracy, precision, recall\n",
    "\n",
    "        model = hyperparameter_search_rf(X_train, y_train, class_weights_dict)\n",
    "        print(f'RF: {evaluate_model(model, X_test, y_test)}')\n",
    "\n",
    "        model = hyperparameter_search_xgb(X_train, y_train)\n",
    "        print(f'Xgb: {evaluate_model(model, X_test, y_test)}')\n",
    "\n",
    "        model = hyperparameter_search_lgb(X_train, y_train, class_weights_dict)\n",
    "        print(f'Light: {evaluate_model(model, X_test, y_test)}')\n",
    "\n",
    "        model = hyperparameter_search_cb(X_train, y_train, class_weights_dict)\n",
    "        print(f'Cat: {evaluate_model(model, X_test, y_test)}')\n",
    "\n",
    "        model = train_svm(X_train, y_train, class_weights_dict, anno)\n",
    "        print(f'SVM: {evaluate_model(model, X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
